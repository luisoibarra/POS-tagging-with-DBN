{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code for POS Tagger using gpmpy's DBN\n",
        "- Luis Ernesto Ibarra C511\n",
        "- Luis Enrique Dalmau C511\n",
        "\n",
        "Original file is located at https://colab.research.google.com/drive/1xVCSM_b_hyrcOKRaJFSohwGlZcfn9sZ7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Downloading corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1eKrDSMlnq5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"brown\")\n",
        "# nltk.download(\"treebank\")\n",
        "# nltk.download(\"conll2000\")\n",
        "\n",
        "nltk.download(\"universal_tagset\")\n",
        "from nltk.corpus import brown#, treebank, conll2000\n",
        "\n",
        "brown_sentences = brown.tagged_sents(tagset=\"universal\")\n",
        "# treebank_sentences = treebank.tagged_sents(tagset=\"universal\")\n",
        "# conll_sentences = conll2000.tagged_sents(tagset=\"universal\")\n",
        "\n",
        "brown_tags = sorted(set(tag for sentence in brown_sentences for _, tag in sentence))\n",
        "# treebank_tags = sorted(set(tag for sentence in treebank_sentences for _, tag in sentence))\n",
        "# conll_tags = sorted(set(tag for sentence in conll_sentences for _, tag in sentence))\n",
        "print(\"Brown sentences:\", len(brown_sentences))\n",
        "# print(\"Treebank sentences:\", len(treebank_sentences))\n",
        "# print(\"Conll2000 sentences:\", len(conll_sentences))\n",
        "print(\"Brown Tags:\", len(brown_tags))\n",
        "# print(\"Treebank Tags:\", len(treebank_tags))\n",
        "# print(\"Conll2000 Tags:\", len(conll_tags))\n",
        "\n",
        "# tags = sorted(set(brown_tags + treebank_tags + conll_tags))\n",
        "tags = sorted(set(brown_tags))\n",
        "print(\"All tags:\", len(tags))\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating train/test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OA7bY9NbBom0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "test_sentences = [\n",
        "    [(\"The\", \"DET\"), (\"cat\", \"NOUN\"), (\"plays\", \"VERB\")],\n",
        "    [(\"The\", \"DET\"), (\"dog\", \"NOUN\"), (\"plays\", \"VERB\")],\n",
        "    [(\"The\", \"DET\"), (\"pretty\", \"ADJ\"), (\"dog\", \"NOUN\"), (\"plays\", \"VERB\")],\n",
        "    [(\"The\", \"DET\"), (\"pretty\", \"ADJ\"), (\"cat\", \"NOUN\"), (\"plays\", \"VERB\")],\n",
        "    [(\"I\", \"NOUN\"), (\"play\", \"VERB\"), (\"with\", \"PRT\"), (\"a\", \"DET\"), (\"cat\", \"NOUN\")],\n",
        "]\n",
        "\n",
        "working_sentences, test_sentences = train_test_split(list(brown_sentences), train_size=0.7, random_state=100)\n",
        "# working_sentences = test_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating DataFrames from corpus splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMbiUq_Drhb7"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "def get_features(word:str):\n",
        "  hyphen = '-' in word\n",
        "  uppercase = word[0].isupper()\n",
        "  return {\n",
        "      \"hyphen\": hyphen,\n",
        "      \"uppercase\": uppercase,\n",
        "      \"numeric\": word.isnumeric(),\n",
        "      \"punctuation\": any(p in word for p in \",.;{}[]()<>/'\\\"!\") \n",
        "  }\n",
        "\n",
        "def get_features_tuples(word):\n",
        "  features = get_features(word)\n",
        "  return sorted(features.items())\n",
        "\n",
        "sentence_info = [(sentence_id, word_index, word, pos_tag, *[val for _,val in get_features_tuples(word)]) # Feature vector\n",
        "                    for sentence_id, sentence in enumerate(working_sentences) \n",
        "                      for word_index, (word, pos_tag) in enumerate(sentence)]\n",
        "\n",
        "df = DataFrame(data=sentence_info, columns=[\n",
        "    \"sentence_id\",\n",
        "    \"word_index\",\n",
        "    \"word\",\n",
        "    \"pos_tag\",\n",
        "    *[name for name, _ in get_features_tuples(\"random word\")]\n",
        "])\n",
        "print(df.head())\n",
        "print(df.describe())\n",
        "\n",
        "def get_bigram_df(df, max=-1):\n",
        "  \"\"\"\n",
        "  Returns a new DataFrame with a single row containing the columns names suffixed with 0 or 1\n",
        "  representing the position on the bigram. \n",
        "  \"\"\"\n",
        "  rows = iter(df.iterrows())\n",
        "  columns = [c for c in df.columns]\n",
        "  result_columns = [f\"{x}_{i}\" for x in columns for i in range(2)]\n",
        "  to_return = { key:[] for key in result_columns }\n",
        "  _,previous = next(rows)\n",
        "  count = 0\n",
        "  for _,row in rows:\n",
        "    if max != -1 and count >= max:\n",
        "      return DataFrame(to_return)\n",
        "    for col_name,value in zip(columns,previous):\n",
        "      to_return[col_name+\"_0\"].append(value)\n",
        "    for col_name,value in zip(columns,row):\n",
        "      to_return[col_name+\"_1\"].append(value)\n",
        "    previous = row\n",
        "    count+=1\n",
        "  return DataFrame(to_return)\n",
        "\n",
        "bigram_df = get_bigram_df(df)\n",
        "\n",
        "print(bigram_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training the model\n",
        "\n",
        "1. Create the model\n",
        "2. Computing the initial table\n",
        "3. Computing the transition table\n",
        "4. Computing the emission table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RaNo_P0Bwp8m"
      },
      "outputs": [],
      "source": [
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
        "from pgmpy.models import BayesianNetwork as BN\n",
        "from pgmpy.inference import DBNInference\n",
        "from pgmpy.estimators import ParameterEstimator, MaximumLikelihoodEstimator, BayesianEstimator\n",
        "import numpy as np\n",
        "\n",
        "verbose = False\n",
        "\n",
        "hidden = {\n",
        "    \"pos_tag\", # Part-Of-Speech tag\n",
        "}\n",
        "\n",
        "hidden_posible_values = {\n",
        "    \"pos_tag\": tags\n",
        "}\n",
        "\n",
        "hidden_card = {\n",
        "    \"pos_tag\": len(hidden_posible_values[\"pos_tag\"]), # Part-Of-Speech tag\n",
        "}\n",
        "\n",
        "observable = set(get_features(\"A\").keys())\n",
        "\n",
        "observable_posible_values = {\n",
        "    \"uppercase\": [False, True],\n",
        "    \"hyphen\": [False, True],\n",
        "    \"punctuation\": [False, True],\n",
        "    \"numeric\": [False, True],\n",
        "}\n",
        "\n",
        "observable_card = {\n",
        "    x: len(observable_posible_values[x]) for x in observable_posible_values\n",
        "}\n",
        "\n",
        "dbnet = DBN()\n",
        "# Connect all hidden variables to observable variables\n",
        "dbnet.add_edges_from([((x, 0), (y, 0)) for x in hidden for y in observable])\n",
        "# Add 1-1 hidden states connections between slices\n",
        "dbnet.add_edges_from([((x, 0), (x, 1)) for x in hidden])\n",
        "# All cpds\n",
        "cpds = []\n",
        "\n",
        "\n",
        "# Get Initial estimator\n",
        "\n",
        "# Initial CPD Info, only sees the initial word of sentences\n",
        "df_init = df[df[\"word_index\"] == 0]\n",
        "\n",
        "# Creating initial Bayesian network\n",
        "init_bayes = BN()\n",
        "# Connect all hidden variables to observable variables\n",
        "init_bayes.add_edges_from([(x, y) for x in hidden for y in observable])\n",
        "\n",
        "# Estimating CPD for Initial Network\n",
        "cpd = MaximumLikelihoodEstimator(init_bayes, df_init)\n",
        "initial_cpd_pos = cpd.estimate_cpd('pos_tag')\n",
        "\n",
        "# Adding number correspondence to tags\n",
        "number_2_pos_dict = initial_cpd_pos.no_to_name['pos_tag']\n",
        "missing_tags = {len(number_2_pos_dict)+i:tag for i, tag in enumerate(set(tags).difference(number_2_pos_dict.values()))}\n",
        "number_2_pos_dict.update(missing_tags)\n",
        "\n",
        "# Fixing missing POS tags in Initial CPD by adding the remaining tags with 0 probability\n",
        "initial_cpd_values = initial_cpd_pos.get_values()\n",
        "pos_probabilities = np.reshape(np.array([x for x in initial_cpd_values] + [x for x in np.zeros(len(tags) - initial_cpd_values.shape[0])]), (len(tags),1))\n",
        "\n",
        "# Initial CPD\n",
        "start_cpd = TabularCPD(\n",
        "    variable=(\"pos_tag\",0),\n",
        "    variable_card=len(tags),\n",
        "    values=pos_probabilities,\n",
        "    evidence=[], \n",
        "    evidence_card=[], \n",
        "    state_names={(\"pos_tag\",0):[number_2_pos_dict[i] for i in range(len(number_2_pos_dict))]}\n",
        "  )\n",
        "if verbose: print(\"Initial distribution\")\n",
        "if verbose: print(start_cpd)\n",
        "\n",
        "# Adding CPD to Final CPDs list\n",
        "cpds.append(start_cpd)\n",
        "\n",
        "# Get 2-TBN estimator\n",
        "\n",
        "two_tbn_bayes = BN()\n",
        "# Connect all hidden variables to observable variables\n",
        "two_tbn_bayes.add_edges_from([(f'{x}_0', f'{y}_0') for x in hidden for y in observable])\n",
        "# Add 1-1 hidden states connections between slices\n",
        "two_tbn_bayes.add_edges_from([(f'{x}_0', f'{x}_1') for x in hidden])\n",
        "\n",
        "cpd = MaximumLikelihoodEstimator(two_tbn_bayes, bigram_df)\n",
        "\n",
        "# Get transitions CPD for Hidden Variables\n",
        "for hid in hidden:\n",
        "  two_tbn_cpd = cpd.estimate_cpd(hid + \"_1\")\n",
        "  # print(two_tbn_cpd)\n",
        "  hid_cardinality = hidden_card[hid]\n",
        "  two_tbn_cpd_no_to_name = two_tbn_cpd.no_to_name\n",
        "  state_names = {\n",
        "          (var[:-2], 0 if var[-1] == \"0\" else 1):[two_tbn_cpd_no_to_name[var][i] for i in range(len(two_tbn_cpd_no_to_name[var]))] \n",
        "              for var in two_tbn_cpd_no_to_name\n",
        "  }\n",
        "\n",
        "  # Values not in Dataset\n",
        "  missing_values = set(hidden_posible_values[hid]).difference(state_names[hid,1])\n",
        "\n",
        "  # Get value matrix\n",
        "  values = two_tbn_cpd.get_values()\n",
        "  for key in state_names:\n",
        "    # Adding missing value names to state names\n",
        "    state_names[key] += [missing for missing in missing_values]\n",
        "\n",
        "  # Filling missing values with 0\n",
        "  values = np.append(values, np.zeros((len(missing_values), values.shape[1])), axis=0)\n",
        "  # Filling missing values with uniform distribution\n",
        "  values = np.append(values, np.full((values.shape[0], len(missing_values)), 1/hidden_card[hid]), axis=1)\n",
        "\n",
        "  two_tbn_cpd = TabularCPD(\n",
        "      variable=(hid,1),\n",
        "      variable_card=hid_cardinality,\n",
        "      values=values, \n",
        "      evidence=[(hid,0)], \n",
        "      evidence_card=[hid_cardinality], \n",
        "      state_names=state_names\n",
        "    )\n",
        "  if verbose: print(f\"Hidden state transition {hid}\")\n",
        "  if verbose: print(two_tbn_cpd)\n",
        "  cpds.append(two_tbn_cpd)\n",
        "\n",
        "# Get CPD for in-slice Variables\n",
        "for observed in observable:\n",
        "  two_tbn_cpd = cpd.estimate_cpd(observed + \"_0\")\n",
        "  observed_cardinality = observable_card[observed]\n",
        "  \n",
        "  two_tbn_cpd_no_to_name = two_tbn_cpd.no_to_name\n",
        "  state_names = {\n",
        "          (var[:-2], 0 if var[-1] == \"0\" else 1):[two_tbn_cpd_no_to_name[var][i] for i in range(len(two_tbn_cpd_no_to_name[var]))] \n",
        "              for var in two_tbn_cpd_no_to_name\n",
        "  }\n",
        "\n",
        "  values = two_tbn_cpd.get_values()\n",
        "  \n",
        "  # Values not in Dataset\n",
        "  missing_values = {\n",
        "      (name,0): set((observable_posible_values if name in observable_posible_values else hidden_posible_values)[name])\n",
        "            .difference(state_names[name,0]) for name,time in state_names \n",
        "  }\n",
        "  \n",
        "  # Get value matrix\n",
        "  values = two_tbn_cpd.get_values()\n",
        "  for key in state_names:\n",
        "    # Adding missing value names to state names\n",
        "    state_names[key] += [missing for missing in missing_values[key]]\n",
        "  \n",
        "  # Filling missing values with 0\n",
        "  values = np.append(values, np.zeros((len(missing_values[observed,0]), values.shape[1])), axis=0)\n",
        "  \n",
        "  hidden_amount = 1\n",
        "  for amount in hidden_card.values():\n",
        "    hidden_amount *= amount\n",
        "\n",
        "  # Filling missing hidden combinations with uniform distribution\n",
        "  values = np.append(values, np.full((values.shape[0], hidden_amount - values.shape[1]), 1/observable_card[observed]), axis=1)\n",
        "\n",
        "  two_tbn_cpd = TabularCPD(\n",
        "      variable=(observed,0),\n",
        "      variable_card= observed_cardinality,\n",
        "      values=values,\n",
        "      evidence=[(hid,0) for hid in hidden], \n",
        "      evidence_card=[hidden_card[hid] for hid in hidden], \n",
        "      state_names=state_names\n",
        "    )\n",
        "  if verbose: print(f\"Observable variable {observed}\")\n",
        "  if verbose: print(two_tbn_cpd)\n",
        "  cpds.append(two_tbn_cpd)\n",
        "\n",
        "\n",
        "dbnet.add_cpds(*cpds)\n",
        "dbnet.initialize_initial_state()\n",
        "\n",
        "# pe = ParameterEstimator(dbnet, df[[\"pos_tag\", \"uppercase\", \"word\", \"hyphen\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Showing the DBN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZkwJ5GZEXef"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import pylab as plt\n",
        "\n",
        "nx.draw(dbnet, with_labels=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1YxZvAxeHSZU"
      },
      "outputs": [],
      "source": [
        "from pgmpy.inference import DBNInference\n",
        "\n",
        "dbn_inf = DBNInference(dbnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsmvutZqIWBc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def analyce_sentence(sentence, verbose=False):\n",
        "\n",
        "  words, pos = [word for word,_ in sentence], [pos for _,pos in sentence]\n",
        "\n",
        "  results = []\n",
        "\n",
        "  sentence_pos_iter = iter(enumerate(zip(words,pos)))\n",
        "  position, (prev_word, prev_pos) = next(sentence_pos_iter)\n",
        "\n",
        "  result = dbn_inf.query([('pos_tag',0)], { # P(POS_0 | *ObservableFeatures)\n",
        "    (name, 0): value for name, value in  get_features(prev_word).items()\n",
        "  })[('pos_tag',0)]\n",
        "\n",
        "  pos_no_to_name = result.no_to_name[(\"pos_tag\",0)]\n",
        "\n",
        "  max_index, val = max(enumerate(result.values), key= lambda x: x[1])\n",
        "  predicted_pos = pos_no_to_name[max_index]\n",
        "\n",
        "  results.append((prev_word, prev_pos, predicted_pos))\n",
        "  if verbose: print(results[-1])\n",
        "\n",
        "  for position, (word, pos) in sentence_pos_iter:\n",
        "    \n",
        "    result = dbn_inf.query([('pos_tag',1)], { # P(POS_1 | POS_0, *ObservableFeatures)\n",
        "        (\"pos_tag\", 0): prev_pos,\n",
        "        **{(name, 1): value for name, value in get_features(word).items()}\n",
        "    })[(\"pos_tag\", 1)]\n",
        "\n",
        "    max_index, val = max(enumerate(result.values), key= lambda x: x[1])\n",
        "\n",
        "    predicted_pos = pos_no_to_name[max_index]\n",
        "\n",
        "    results.append((word, pos, predicted_pos))\n",
        "    if verbose: print(results[-1])\n",
        "    prev_pos = pos\n",
        "\n",
        "  positive = 0\n",
        "  results_table = {\n",
        "      \"word\":[],\n",
        "      \"true_pos\":[],\n",
        "      \"pred_pos\":[],\n",
        "      \"prev_word\":[]\n",
        "  }\n",
        "\n",
        "  for index, (word, true_pos, pred_pos) in enumerate(results):\n",
        "    if true_pos == pred_pos:\n",
        "      positive += 1\n",
        "    results_table[\"word\"].append(word)\n",
        "    results_table[\"true_pos\"].append(true_pos)\n",
        "    results_table[\"pred_pos\"].append(pred_pos)\n",
        "    results_table[\"prev_word\"].append(\"\" if index == 0 else results[index-1][0])\n",
        "\n",
        "  return positive, len(results), DataFrame(results_table)\n",
        "\n",
        "def analyce_sentences(sentences):\n",
        "  total = 0\n",
        "  positive = 0\n",
        "  prev_df = None\n",
        "  for sentence in sentences:\n",
        "    new_positive, new_total, df = analyce_sentence(sentence)\n",
        "    positive += new_positive\n",
        "    total += new_total\n",
        "    if prev_df is None:\n",
        "      prev_df = df\n",
        "    else:\n",
        "      prev_df = prev_df.append(df)\n",
        "    print(\"Positive\", positive)\n",
        "    print(\"Total\", total)\n",
        "    print(\"Precision\", positive/total)\n",
        "  return positive, total, prev_df\n",
        "\n",
        "# print(\"Train Acc\")\n",
        "# train_positive, train_total, results = analyce_sentences(working_sentences)\n",
        "\n",
        "print(\"Test Acc\")\n",
        "test_positive, test_total, results = analyce_sentences(test_sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Showing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcgb847AamoD"
      },
      "outputs": [],
      "source": [
        "print(\"Results\")\n",
        "print(\"Test positive\", test_positive)\n",
        "print(\"Test total\", test_total)\n",
        "print(\"Test precision\", test_positive/test_total)\n",
        "results.describe()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mgp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
